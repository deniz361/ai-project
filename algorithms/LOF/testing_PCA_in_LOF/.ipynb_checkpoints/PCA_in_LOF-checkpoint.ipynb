{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3bd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raoul\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Material number  Supplier      Contract  Contract Position  \\\n",
      "153197      23884900UB    700000           NaN                  0   \n",
      "182615        00360029    701806  5.500001e+09                 10   \n",
      "140446        04002905    706808           NaN                  0   \n",
      "164016        60022412      1000           NaN                  0   \n",
      "175162      01568000UB    700000           NaN                  0   \n",
      "\n",
      "        Fulfillment time  Fixed contract 1  Fixed contract 2 Procurement type  \\\n",
      "153197               NaN               NaN               NaN                F   \n",
      "182615               4.0               4.0              60.0                F   \n",
      "140446               NaN               NaN               NaN                F   \n",
      "164016               NaN               NaN               NaN                F   \n",
      "175162               NaN               NaN               NaN                F   \n",
      "\n",
      "        Special procurement type  Dispatcher  ...  Price unit Calendar Plant  \\\n",
      "153197                       NaN         723  ...           1      E20  2000   \n",
      "182615                       NaN         708  ...         100      E98  2100   \n",
      "140446                       NaN         272  ...           1      NaN  3000   \n",
      "164016                       NaN         991  ...           1      NaN  2100   \n",
      "175162                       NaN         223  ...           1      E19  2100   \n",
      "\n",
      "        Plant information record  Information record number  \\\n",
      "153197                       NaN               5.300024e+09   \n",
      "182615                       NaN               5.300174e+09   \n",
      "140446                       NaN               5.300107e+09   \n",
      "164016                       NaN               5.300376e+09   \n",
      "175162                       NaN               5.300179e+09   \n",
      "\n",
      "        Information record type Plant processing time  Material master time  \\\n",
      "153197                      0.0                   1.0                    56   \n",
      "182615                      0.0                   2.0                     4   \n",
      "140446                      0.0                   1.0                    14   \n",
      "164016                      0.0                   1.0                    42   \n",
      "175162                      0.0                   1.0                    64   \n",
      "\n",
      "        Product group  Base unit  \n",
      "153197       10401000         ST  \n",
      "182615      140100000         ST  \n",
      "140446      130600000         ST  \n",
      "164016       10402000         ST  \n",
      "175162       10401000         ST  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "  Material number  Supplier      Contract  Contract Position  \\\n",
      "0      23884900UB    700000           NaN                  0   \n",
      "1        00360029    701806  5.500001e+09                 10   \n",
      "2        04002905    706808           NaN                  0   \n",
      "3        60022412      1000           NaN                  0   \n",
      "4      01568000UB    700000           NaN                  0   \n",
      "\n",
      "   Fulfillment time  Fixed contract 1  Fixed contract 2 Procurement type  \\\n",
      "0               NaN               NaN               NaN                F   \n",
      "1               4.0               4.0              60.0                F   \n",
      "2               NaN               NaN               NaN                F   \n",
      "3               NaN               NaN               NaN                F   \n",
      "4               NaN               NaN               NaN                F   \n",
      "\n",
      "   Special procurement type  Dispatcher  ...  Price unit Calendar Plant  \\\n",
      "0                       NaN         723  ...           1      E20  2000   \n",
      "1                       NaN         708  ...         100      E98  2100   \n",
      "2                       NaN         272  ...           1      NaN  3000   \n",
      "3                       NaN         991  ...           1      NaN  2100   \n",
      "4                       NaN         223  ...           1      E19  2100   \n",
      "\n",
      "   Plant information record  Information record number  \\\n",
      "0                       NaN               5.300024e+09   \n",
      "1                       NaN               5.300174e+09   \n",
      "2                       NaN               5.300107e+09   \n",
      "3                       NaN               5.300376e+09   \n",
      "4                       NaN               5.300179e+09   \n",
      "\n",
      "   Information record type Plant processing time  Material master time  \\\n",
      "0                      0.0                   1.0                    56   \n",
      "1                      0.0                   2.0                     4   \n",
      "2                      0.0                   1.0                    14   \n",
      "3                      0.0                   1.0                    42   \n",
      "4                      0.0                   1.0                    64   \n",
      "\n",
      "   Product group  Base unit  \n",
      "0       10401000         ST  \n",
      "1      140100000         ST  \n",
      "2      130600000         ST  \n",
      "3       10402000         ST  \n",
      "4       10401000         ST  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "\n",
    "def sample_and_save_data(original_data, sample_fraction=0.1, random_state=42, file_path='sampled_data.csv'):\n",
    "    \"\"\"\n",
    "    Sample the original data, save it to a CSV file, and return the sampled DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - original_data: pandas DataFrame containing the original dataset\n",
    "    - sample_fraction: Fraction of the original data to sample (default=0.01)\n",
    "    - random_state: Random state for reproducibility (default=42)\n",
    "    - file_path: File path to save the sampled data (default='sampled_data.csv')\n",
    "\n",
    "    Returns:\n",
    "    - sampled_data: pandas DataFrame containing the sampled dataset\n",
    "    \"\"\"\n",
    "    sampled_data = original_data.sample(frac=sample_fraction, random_state=random_state)\n",
    "    sampled_data.to_csv(file_path, index=False)\n",
    "    return sampled_data\n",
    "\n",
    "def load_data_and_run_lof(file_path, n_neighbors=10, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Load the sampled data from a CSV file, run the Local Outlier Factor (LOF) algorithm,\n",
    "    and return anomalies detected along with the LOF scores.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: File path to load the sampled data from\n",
    "    - n_neighbors: Number of neighbors to consider for LOF calculation (default=20)\n",
    "    - contamination: Expected proportion of outliers in the dataset (default=0.1)\n",
    "\n",
    "    Returns:\n",
    "    - anomalies: pandas DataFrame containing the anomalies identified by LOF\n",
    "    - lof_scores: LOF scores for each data point\n",
    "    \"\"\"\n",
    "    # Load the sampled data\n",
    "    sampled_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Preprocess the sampled data\n",
    "    processed_sampled_data = preprocess_step1(sampled_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    processed_sampled_data, reduced_data = preprocess_step2(processed_sampled_data)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Detect anomalies from reduced data\n",
    "    reduced_data_anomalies, reduced_data_lof_scores = detect_anomalies_LOF(reduced_data, n_neighbors=n_neighbors, contamination=contamination)\n",
    "    end = time.time()\n",
    "    print(\"Reduced processing time: \", end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Detect anomalies using LOF\n",
    "    anomalies, lof_scores = detect_anomalies_LOF(processed_sampled_data, n_neighbors=n_neighbors, contamination=contamination)\n",
    "    end = time.time()\n",
    "    print(\"Regular processing time: \", end-start)\n",
    "    \n",
    "    return anomalies, lof_scores, reduced_data_anomalies, reduced_data_lof_scores\n",
    "\n",
    "def preprocess_step1(data):\n",
    "    \"\"\"\n",
    "    Preprocess the data by imputing missing values, performing one-hot encoding for categorical variables,\n",
    "    and performing feature normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset\n",
    "\n",
    "    Returns:\n",
    "    - processed_data: pandas DataFrame containing imputed missing values, one-hot encoded features, and normalized features\n",
    "    \"\"\"\n",
    "    # Rename all columns to English\n",
    "    data.rename(columns={\"Materialnummer\": \"Material number\", \"Lieferant OB\": \"Supplier\", \"Vertrag OB\": \"Contract\", \n",
    "                         \"Vertragsposition OB\": \"Contract Position\", \"Planlieferzeit Vertrag\": \"Fulfillment time\", \n",
    "                         \"Vertrag Fix1\": \"Fixed contract 1\", \"Vertrag_Fix2\": \"Fixed contract 2\", \"Beschaffungsart\": \n",
    "                         \"Procurement type\", \"Sonderbeschaffungsart\": \"Special procurement type\", \"Disponent\":\n",
    "                         \"Dispatcher\", \"Einkäufer\": \"Buyer\", \"DispoGruppe\": \"Purchasing group\", \"Dispolosgröße\": \n",
    "                         \"Purchasing lot size\", \"Gesamtbestand\": \"Total quantity\", \"Gesamtwert\": \"Total value\",\n",
    "                         \"Preiseinheit\": \"Price unit\", \"Kalender\": \"Calendar\", \"Werk OB\": \"Plant\", \"Werk Infosatz\":\n",
    "                         \"Plant information record\", \"Infosatznummer\": \"Information record number\", \"Infosatztyp\":\n",
    "                         \"Information record type\", \"WE-Bearbeitungszeit\": \"Plant processing time\", \"Planlieferzeit Mat-Stamm\":\n",
    "                         \"Material master time\", \"Warengruppe\": \"Product group\", \"Basiseinheit\": \"Base unit\"}, inplace=True)\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "    \n",
    "    # Replace 0s with NaN in specific columns\n",
    "    columns_to_replace_nan = [\"Fulfillment time\", \"Material master time\", \"Plant processing time\", \"Total quantity\", \"Total value\", \"Fixed contract 1\", \"Fixed contract 2\"]\n",
    "    data[columns_to_replace_nan] = data[columns_to_replace_nan].replace(0, np.nan)\n",
    "    print (data.head())\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_step2(data):\n",
    "    numeric_cols = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "    # Impute missing values using mean imputation for numeric columns\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data_numeric_imputed = pd.DataFrame(imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_cols = [\"Material number\", \"Supplier\", \"Contract Position\", \"Procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing lot size\", \"Plant\", \"Product group\", \"Base unit\"]\n",
    "    if len(categorical_cols) > 0:\n",
    "        encoder = OneHotEncoder(drop='first')\n",
    "        data_encoded = encoder.fit_transform(data[categorical_cols])\n",
    "        column_names = encoder.get_feature_names_out(categorical_cols)\n",
    "        data_imputed_encoded = pd.DataFrame(data_encoded.toarray(), columns=column_names)\n",
    "    else:\n",
    "        data_imputed_encoded = pd.DataFrame()\n",
    "\n",
    "    # Combine numeric and encoded categorical columns\n",
    "    processed_data = pd.concat([data_numeric_imputed, data_imputed_encoded], axis=1)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    processed_data = pd.DataFrame(scaler.fit_transform(processed_data), columns=processed_data.columns)\n",
    "    \n",
    "    create_poc_graph(processed_data)\n",
    "    \n",
    "    print(\"\\n\", \"processed_data: \")\n",
    "    print(processed_data)\n",
    "    \n",
    "    reduced_dataframe = get_reduced_dataframe(processed_data)\n",
    "\n",
    "    return processed_data, reduced_dataframe\n",
    "\n",
    "def detect_anomalies_LOF(data, n_neighbors=10, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Local Outlier Factor (LOF) algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset\n",
    "    - n_neighbors: Number of neighbors to consider for LOF calculation (default=20)\n",
    "    - contamination: Expected proportion of outliers in the dataset (default=0.1)\n",
    "\n",
    "    Returns:\n",
    "    - anomalies: pandas DataFrame containing the anomalies identified by LOF\n",
    "    - lof_scores: LOF scores for each data point\n",
    "    \"\"\"\n",
    "    # Initialize LOF\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "    # Fit LOF to the data and predict anomaly scores\n",
    "    lof_scores = lof.fit_predict(data)\n",
    "\n",
    "    # Create a boolean mask to identify anomalies (outliers have a value of -1)\n",
    "    anomalies_mask = lof_scores == -1\n",
    "\n",
    "    # Filter anomalies\n",
    "    anomalies = data[anomalies_mask]\n",
    "\n",
    "    return anomalies, lof_scores\n",
    "\n",
    "def get_reduced_dataframe(one_hot_encoded_data):\n",
    "    pca = PCA(n_components=10)\n",
    "    reduced_data = pca.fit_transform(one_hot_encoded_data)\n",
    "    print(\"Reduced Data:\")\n",
    "    print(reduced_data[:25])\n",
    "    return reduced_data\n",
    "\n",
    "# Load the original dataset\n",
    "file_path = r'C:\\Users\\Raoul\\Desktop\\Studium\\Semester_10\\git_repo\\ai-project\\datasets/Stammdaten.csv'\n",
    "original_data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Sample and save the data\n",
    "sampled_data = sample_and_save_data(original_data)\n",
    "processed_data = preprocess_step1(sampled_data)\n",
    "processed_data.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "# Load the sampled data, run LOF, and detect anomalies\n",
    "anomalies, lof_scores, reduced_data_anomalies, reduced_data_lof_scores = load_data_and_run_lof('sampled_data.csv')\n",
    "\n",
    "# Create a new column \"anomaly\" in the sampled data DataFrame\n",
    "processed_data['anomaly'] = np.where(lof_scores < 0, 1, 0)\n",
    "processed_data['reduced_data_anomaly'] = np.where(reduced_data_lof_scores < 0, 1, 0)\n",
    "print(processed_data.head())\n",
    "# Save the sampled data with the \"anomaly\" column as CSV\n",
    "processed_data.to_csv('processed_data_with_anomalies.csv', index=False)\n",
    "\n",
    "\n",
    "# Print indices of the sampled data DataFrame\n",
    "print(\"Indices of sampled data:\")\n",
    "print(sampled_data.index)\n",
    "\n",
    "# Print indices of the anomalies DataFrame\n",
    "print(\"\\nIndices of anomalies:\")\n",
    "print(anomalies.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5e6c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indices of anomalies:\n",
      "Index([    2,    10,    28,    34,    39,    50,    51,    60,    63,    69,\n",
      "       ...\n",
      "       18605, 18606, 18617, 18628, 18633, 18637, 18640, 18645, 18646, 18669],\n",
      "      dtype='int64', length=1868)\n",
      "\n",
      "Indices of pca anomalies:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(anomalies\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIndices of pca anomalies:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(reduced_data_anomalies\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "print(\"\\nIndices of anomalies:\")\n",
    "print(anomalies.index)\n",
    "\n",
    "print(\"\\nIndices of pca anomalies:\")\n",
    "print(reduced_data_anomalies.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3563a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of matching entries: 81.41%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of matching entries\n",
    "matching_percentage = (processed_data['anomaly'] == processed_data['reduced_data_anomaly']).mean() * 100\n",
    "\n",
    "print(f\"Percentage of matching entries: {matching_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dd6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def create_poc_graph(processed_data):\n",
    "    # Assuming X is your data matrix\n",
    "    pca = PCA().fit(processed_data)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "    plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance Explained')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81e9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
