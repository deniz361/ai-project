{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5693fbe1-6b97-4a31-b6f4-6965d8a7b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 21:20:06.882671: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 21:20:07.005513: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 21:20:07.326093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 21:20:09.006434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c323542-64a2-45e4-ba24-8bcb993703c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "validation_df = pd.read_csv('val.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['anomaly'])\n",
    "y_train = train_df['anomaly']\n",
    "X_test = test_df.drop(columns=['anomaly'])\n",
    "y_test = test_df['anomaly']\n",
    "X_validation = validation_df.drop(columns=['anomaly'])\n",
    "y_validation = validation_df['anomaly']\n",
    "\n",
    "# Columns\n",
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                       \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                       \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                       \"Information record number\", \"Information record type\", \"Product group\",\n",
    "                       \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                     \"Price unit\", \"Plant processing time\", \"Material master time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b967cfff-16b5-4569-a0bc-6f4e673c3ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.9 GiB for an array with shape (158742, 107391) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m X_validation_cat \u001b[38;5;241m=\u001b[39m X_validation[categorical_columns]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Simple one-hot encoding before feeding to autoencoder\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m X_train_cat_enc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m X_test_cat_enc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X_test_cat)\n\u001b[1;32m     22\u001b[0m X_validation_cat_enc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X_validation_cat)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:214\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[1;32m    224\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 15.9 GiB for an array with shape (158742, 107391) and data type bool"
     ]
    }
   ],
   "source": [
    "# Preprocess the Data\n",
    "# Normalize Numerical Features\n",
    "# Use Autoencoder for Categorical Features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_num = scaler.transform(X_test[numerical_columns])\n",
    "X_validation_num = scaler.transform(X_validation[numerical_columns])\n",
    "\n",
    "\n",
    "\n",
    "# Train an autoencoder for the categorical features\n",
    "# Convert categorical columns to strings for encoding\n",
    "\n",
    "X_train_cat = X_train[categorical_columns].astype(str)\n",
    "X_test_cat = X_test[categorical_columns].astype(str)\n",
    "X_validation_cat = X_validation[categorical_columns].astype(str)\n",
    "\n",
    "# Simple one-hot encoding before feeding to autoencoder\n",
    "X_train_cat_enc = pd.get_dummies(X_train_cat)\n",
    "X_test_cat_enc = pd.get_dummies(X_test_cat)\n",
    "X_validation_cat_enc = pd.get_dummies(X_validation_cat)\n",
    "\n",
    "# Align the columns in case some categories are missing in some sets\n",
    "X_train_cat_enc, X_test_cat_enc = X_train_cat_enc.align(X_test_cat_enc, fill_value=0, axis=1)\n",
    "X_train_cat_enc, X_validation_cat_enc = X_train_cat_enc.align(X_validation_cat_enc, fill_value=0, axis=1)\n",
    "\n",
    "# Autoencoder structure\n",
    "input_dim = X_train_cat_enc.shape[1]\n",
    "encoding_dim = 10  # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train_cat_enc, X_train_cat_enc, \n",
    "                epochs=50, \n",
    "                batch_size=256, \n",
    "                shuffle=True, \n",
    "                validation_data=(X_validation_cat_enc, X_validation_cat_enc))\n",
    "\n",
    "# Get the encoder part\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "\n",
    "# Encode the categorical features\n",
    "X_train_cat_enc = encoder_model.predict(X_train_cat_enc)\n",
    "X_test_cat_enc = encoder_model.predict(X_test_cat_enc)\n",
    "X_validation_cat_enc = encoder_model.predict(X_validation_cat_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fbe09-bb86-44a9-b467-a2125622e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the Processed Features\n",
    "# Combine the numerical and encoded categorical features\n",
    "X_train_final = np.hstack((X_train_num, X_train_cat_enc))\n",
    "X_test_final = np.hstack((X_test_num, X_test_cat_enc))\n",
    "X_validation_final = np.hstack((X_validation_num, X_validation_cat_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853a6ce-5d78-4985-8aab-84f6d133e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost Model\n",
    "# Initialize the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_final)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55659d84-c7e4-4cc7-bd2d-b6341d9ba0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Model\n",
    "y_val_pred = model.predict(X_validation_final)\n",
    "val_accuracy = accuracy_score(y_validation, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ee5f5-87de-467d-befa-ad4fd1c9b586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680068e-7fb5-4703-bc3f-8d6b4d2b8b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5973b9-814a-4b29-a8a9-83c129dc4c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f61220-2a5a-44f9-8ad9-a2fb070a9b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bf723-0d80-4309-83bf-8e75b81e6f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780bd96-5028-40b0-8de8-9e352c326e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036efb0-67c3-4ea4-8c60-fa92d951460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd1df6-92cd-4d61-a292-fa187805c44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad7b0e-1c2a-42f8-835f-052320c08ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705734c-659b-49dd-b09e-a0385d7db565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbeb39-baff-472f-984d-3dd121d9d63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb7084-1245-44b1-924e-885e2074946d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1971a0e9-e743-4e09-8d50-e7e9028005b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:28:19.182465: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 21:28:19.216081: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 21:28:19.316370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 21:28:20.397632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gravityfall_kevin/.conda/envs/ai/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-10 21:28:24.898606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 360396000 exceeds 10% of free system memory.\n",
      "2024-06-10 21:28:27.053528: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 360396000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m35/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:28:31.474752: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 40044000 exceeds 10% of free system memory.\n",
      "2024-06-10 21:28:31.603436: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 40044000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The layer sequential has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mfit(categorical_data_encoded\u001b[38;5;241m.\u001b[39mvalues, categorical_data_encoded\u001b[38;5;241m.\u001b[39mvalues, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Step 5: Encode Categorical Data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m, outputs\u001b[38;5;241m=\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     40\u001b[0m encoded_categorical_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(categorical_data_encoded)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Step 6: Combine Encoded Categorical Data with Numerical Data\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/keras/src/ops/operation.py:228\u001b[0m, in \u001b[0;36mOperation.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/keras/src/ops/operation.py:259\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The layer sequential has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "file_path = 'supervised_dataset_chunk_part_1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.shape)\n",
    "# Step 2: Define Categorical and Numerical Columns\n",
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                       \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                       \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                       \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                       \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                     \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "\n",
    "categorical_data = data[categorical_columns]\n",
    "categorical_data_encoded = pd.get_dummies(categorical_data).astype('float32')\n",
    "input_dim = categorical_data_encoded.shape[1]\n",
    "\n",
    "### Step 4: Define and Train the Autoencoder\n",
    "encoding_dim = 32  # Dimension of the encoded representation\n",
    "autoencoder = Sequential([\n",
    "    Dense(encoding_dim, input_shape=(input_dim,), activation='relu'),\n",
    "    Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(categorical_data_encoded.values, categorical_data_encoded.values, epochs=20, batch_size=256, shuffle=True, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Step 5: Encode Categorical Data\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[0].output)\n",
    "encoded_categorical_data = encoder.predict(categorical_data_encoded)\n",
    "\n",
    "# Step 6: Combine Encoded Categorical Data with Numerical Data\n",
    "data_combined = np.concatenate((data[numerical_columns].values, encoded_categorical_data), axis=1)\n",
    "\n",
    "# Step 7: Define Features and Target\n",
    "X = data_combined\n",
    "y = data['anomaly']\n",
    "\n",
    "# Step 8: Train XGBoost Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Identify Anomalies and Top 8 Features\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "# Ensure we are only summing numerical columns when counting anomalies\n",
    "anomalies_indices = np.where(y_pred == 1)[0]  # Indices of detected anomalies\n",
    "\n",
    "# Filter data to include only numerical columns for anomaly summing\n",
    "numerical_data = data[numerical_columns]  # Assuming 'numerical_columns' is predefined list of your numeric columns\n",
    "\n",
    "# Count anomalies in each numerical column\n",
    "anomalies_count = {col: np.sum(numerical_data.iloc[anomalies_indices][col]) for col in numerical_columns}  # Count anomalies in each column\n",
    "\n",
    "# Get the top 8 features with the most anomalies\n",
    "top_8_anomaly_features = sorted(anomalies_count, key=anomalies_count.get, reverse=True)[:8]\n",
    "\n",
    "\n",
    "for feature in top_8_anomaly_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Drop NaN values from the column\n",
    "    feature_data = data[feature].dropna()\n",
    "    if feature_data.empty:\n",
    "        print(f\"No data available to plot for {feature}.\")\n",
    "    else:\n",
    "        plt.hist(feature_data, bins=50, label=f'Distribution of {feature}')\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e99e6-43c2-4b21-9409-44ab123be722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c81fe2-9b5b-44f4-a7ff-d3e391c85e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43fd8a-9b7f-4946-9b39-6fcecf7b9667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
